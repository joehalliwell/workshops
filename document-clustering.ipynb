{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Clustering\n",
    "\n",
    "## Why we're here\n",
    "\n",
    "Goals:\n",
    "\n",
    "- Pique your curiosity about Natural Language Processing (NLP)\n",
    "- Introduce core tools in the Python NLP/Data Science ecosystem\n",
    "- Cluster some documents\n",
    "- Help your neighbours!\n",
    "- Have fun!\n",
    "\n",
    "Non-goals:\n",
    "\n",
    "- Finish the whole thing (but you can certainly try...)\n",
    "\n",
    "## Before the workshop: setup your Python environment\n",
    "\n",
    "Run the following commands in your shell to setup your Python environment.\n",
    "\n",
    "This takes a while, so hopefully you did this before the workshop... but if not you can try to do it now.\n",
    "\n",
    "```bash\n",
    "pip install jupyter matplotlib scipy scikit-learn spacy wordcloud # Install required Python modules\n",
    "spacy download en_core_web_md # Download the Spacy model\n",
    "python -c \"__import__('sklearn.datasets').datasets.fetch_20newsgroups()\" # Download the corpus\n",
    "```\n",
    "\n",
    "## At the start of the workshop: setup your Jupyter notebook\n",
    "\n",
    "Click on the below \"cell\" and hit \"Ctrl-Return\" (Windows/Linux) or \"Command-Return\" (Max) to run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the standard toolkit...\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# ...and switch on \"in notebook\" charts, and make them a bit bigger!\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "print(\"Reticulating splines... DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over to you: hipster business name generator\n",
    "\n",
    "Generally, you're just going go through this notebook executing the code blocks as you did above.\n",
    "\n",
    "But you'll also find a few \"Over to you\" sections scattered throughout. These are a prompt for you to experiment and try things out.\n",
    "\n",
    "To get you started just follow the instructions after the big comment `#####` below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_hipster_biz_name(a, b):\n",
    "    from codecs import decode\n",
    "    from IPython.display import HTML\n",
    "    ns = 'mrcule/zvak/pbj/pbea/cvtrba/funpxyr/obngzna/pbyyne'\n",
    "    qs = 'zbhfgnpur/nagvdhr/pebpurgrq/negvfnany'\n",
    "    bs = 'pbzof/glcrjevgref/fyvccref/ancxvaf/jubyrsbbqf'\n",
    "    def tr(n, c):\n",
    "        c = decode(c, \"rot13\").split('/')\n",
    "        return c[n.__hash__() % len(c)].title()\n",
    "    n = \"{} & {} {} {}\".format(tr(a, ns), tr(b, ns), tr(a + b, qs), tr(b + a, bs))\n",
    "    s = \"font-family:serif;font-size:28pt;text-align:center;border:4px solid black;padding:10px;\"\n",
    "    print(\"Your Hipster business name is:\")\n",
    "    display(HTML(\"<h1 style='{}'>{}</h1>\".format(s, n)))\n",
    "\n",
    "########################################################################################\n",
    "# Edit the line below to include your name and your neighbour's name, then run the cell\n",
    "########################################################################################\n",
    "\n",
    "show_hipster_biz_name(\"Your Name\", \"Your Neighbour's Name\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over to you: getting comfortable with Jupyter \n",
    "\n",
    "- Click the `+` in the toolbar to create a new cell (make sure that it's marked as \"Code\" in the dropdown)\n",
    "- If you want, use the up and down arrow buttons to move it up and down\n",
    "- Enter some Python code (if you don't know any try `1 + 2`, or ask your neighbour)\n",
    "- Click the `Run` button (or Ctrl-Enter) to execute it\n",
    "- Write some code to print the result of multiplying `1337` with `1337` (Hint: use `print(...)`)\n",
    "\n",
    "If you're running the right version of Jupyter, you can also get a guided tour of the interface from the `Help > User Interface Tour` menu option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing SpaCy\n",
    "\n",
    "Okay, let's do some Natural Language Processing (NLP)!\n",
    "\n",
    "SpaCy is a relative newcomer to the NLP scene, but has made a big splash because:\n",
    "- it's fast\n",
    "- it ships with high-performance models for a few different languages\n",
    "- it's got a very nice API\n",
    "\n",
    "They also have great marketing: https://spacy.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up the english language models that we downloaded above and print out some info about them\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "print(\"{name}: {description}\".format(**nlp.meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay let's use SpaCy to process a simple sentence\n",
    "# The fundamental operation is to create a stuctured \"Doc\" representation of a text...\n",
    "\n",
    "text = u\"Pack my bag with twelve dozen liquor jugs.\"\n",
    "doc = nlp(text)\n",
    "doc.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But since we're in Jupyter we can do a lot better than that!\n",
    "# The \"Parts of Speech\" e.g. VERB are drawn from the \"Universal POS Tag\" vocabulary\n",
    "# Find out more at http://universaldependencies.org/u/pos/\n",
    "\n",
    "options={'jupyter': True, 'options':{'distance': 120}}\n",
    "displacy.render(doc, style='dep', **options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy also ships with an \"entity recogniser\" -- it's pretty good!\n",
    "\n",
    "ghostbusters = nlp(u\"In the eponymous 1984 film, New York City celebrated the Ghostbusters with a ticker tape parade.\")\n",
    "displacy.render(ghostbusters, style=\"ent\", **options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over to you: SpaCy exploration!\n",
    "\n",
    "- How does SpaCy handle \"The cat ate the fish\"? Is it correct?\n",
    "- How about \"The old man the boat\"?\n",
    "- And \"The complex houses married and single soldiers and their families\"? What's going on?\n",
    "- Who are the people in \"Saint John met Gina St. John in St John's Wood\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to explore!\n",
    "\n",
    "displacy.render(nlp(u\"Explore Spacy here!\"), style='dep', **options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document vectors\n",
    "\n",
    "For Machine Learning applications it's *often* the case that we want to process a document into a list of numbers or \"vector\".\n",
    "\n",
    "It's worth noting that there are many different ways to do this. Also recent advances in \"deep learning\" as well as providing new ways to generate document vectors also offer ways to work directly with the source text.\n",
    "\n",
    "But for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The larger SpaCy models contain a list of words and their corresponding vectors\n",
    "\n",
    "print(\"Document vectors have {} dimensions\".format(len(doc.vector)))\n",
    "print(\"And are not normalized e.g. this has length {}\".format(np.linalg.norm(doc.vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document vectors capture an intuitive notion of similarity\n",
    "# Words that appear similar contexts are considered similar\n",
    "\n",
    "def print_comparison(a, b):\n",
    "    a = nlp(unicode(a))\n",
    "    b = nlp(unicode(b))\n",
    "    # Euclidean \"L2\" distance\n",
    "    distance = np.linalg.norm(a.vector - b.vector)\n",
    "    # Cosine similarity\n",
    "    similarity = a.similarity(b)\n",
    "    print(\"-\" * 80)\n",
    "    print(\"A: {}\\nB: {}\\nDistance: {}\\nSimilarity: {}\".format(a, b, distance, similarity))\n",
    "\n",
    "text = \"The cat sat on the mat\"\n",
    "print_comparison(text, \"The feline lay on the carpet\")\n",
    "print_comparison(text, \"Three hundred Theban warriors died that day\")\n",
    "print_comparison(text, \"Ceci n'est pas une pipe\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over to you: comparison shopping\n",
    "\n",
    "- How does \"cat\" compare with \"feline\"?\n",
    "- How does \"good\" compare with \"goods\"? How does it compare with \"bad\"? What's going on?\n",
    "- How does \"teh\" compare with \"the\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to explore!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document vectors often also have a very interesting property sometimes called \"linear substructure\"\n",
    "# Basically you can do arithmetic with words/concepts!\n",
    "\n",
    "def vectorize(text):\n",
    "    return nlp(unicode(text), disable=['parser', 'tagger', 'ner']).vector\n",
    "\n",
    "from heapq import heappush, nsmallest, nlargest\n",
    "\n",
    "def get_top_n(target_v, n=5):\n",
    "    \"\"\"Figure out the top-N words most similar to the target vector\"\"\"\n",
    "    heap = []\n",
    "    # SpaCy has a long list of words in `vocab` which we can pick from!\n",
    "    for word in nlp.vocab:\n",
    "        # Filter out mixed case and uncommon terms\n",
    "        if not word.is_lower or word.prob < -15:\n",
    "            continue\n",
    "        distance = np.linalg.norm(target_v - word.vector)\n",
    "        heappush(heap, (distance, word.text))\n",
    "    return nsmallest(n, heap)\n",
    "\n",
    "\n",
    "PUPPY, DOG, KITTEN = [vectorize(w) for w in (\"puppy\", \"dog\", \"kitten\")]\n",
    "\n",
    "get_top_n(DOG - PUPPY + KITTEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can generalize that into a cute analogy finder\n",
    "\n",
    "def print_analogy(a, b, c):\n",
    "    \"\"\"A is to B as C is to ???\"\"\"\n",
    "    top_n = get_top_n(vectorize(b) - vectorize(a) + vectorize(c))\n",
    "    best = [w for (s,w) in top_n if w not in (a,b,c)][0]\n",
    "    print(\"{} is to {} as {} is to {}\".format(a, b, c, best))\n",
    "    \n",
    "print_analogy(\"queen\", \"king\", \"woman\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over to you: an analogy is an idea with another idea's hat on\n",
    "\n",
    "- \"Boy\" is to \"girl\" as \"prince\" is to what?\n",
    "- \"Red\" is to \"reddest\" as \"blue\" is to what?\n",
    "- What is to \"simile\" as \"analogy\" is to \"metaphor\"? (Sorry for the brain strain, but it's good to stretch a bit no?)\n",
    "- Find an example that doesn't work! (It's not too hard :))\n",
    "- (Advanced) Try changing the \"-15\" in `get_top_n` to \"-100\". What effect is it having on your examples?\n",
    "- (Advanced) Could you adapt this code to find \"opposites\" for words?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to explore!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and visualizing documents\n",
    "\n",
    "Okay, so that's a quick introduction to SpaCy. Let's look at some documents.\n",
    "\n",
    "Scikit Learn (sklearn) is a brilliant python library for machine learning. You can find out more about it at http://scikit-learn.org\n",
    "\n",
    "It also ships with some handy features for downloading and reading in some standard pedagogical datasets. Let's have a look at the newsgroups collection of documents (or \"corpus\") that we used Scikit Learn to download earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "raw_posts = fetch_20newsgroups()\n",
    "\n",
    "print(\"Number of posts: {}\".format(len(raw_posts.data)))\n",
    " # Source groups are listed in `target_names`\n",
    "print(\"Newsgroups: {}\".format(raw_posts.target_names))\n",
    " # Post text is in `data`\n",
    "print(\"Sample post text:\\n{0}\\n{1}\\n{0}\".format('-' * 80, raw_posts.data[19]))\n",
    " # Post group is encoded in `target` as an index into `target_names`\n",
    "print(\"Sample post group: {}\".format(raw_posts.target_names[raw_posts.target[19]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's quite a lot of junk in there, headers etc. Fortunately sklearn can help a bit...\n",
    "# We can pass through a special argument to strip out headers, footers and inline quotes\n",
    "\n",
    "raw_posts = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))\n",
    "print(\"Sample post text:\\n{0}\\n{1}\\n{0}\".format('-' * 80, raw_posts.data[19]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another key tool in the Python ecosystem is Pandas which is a library for working with tables of data\n",
    "# We're going to convert the dataset in to a Panda DataFrame for ease of manipulation\n",
    "# Don't worry too much about this -- it's not really our focus today -- but if you're interested you can\n",
    "# find out more at http://pandas.pydata.org/\n",
    "\n",
    "posts = pd.DataFrame({'text': raw_posts.data, 'group': [raw_posts.target_names[t] for t in raw_posts.target]})\n",
    "\n",
    "# Many tools in the Python ecosystem are quite tightly integrated, so once we have DataFrame we can\n",
    "# do things like plot it via the standard charting tool `matplotlib` which we importes as `plt` earlier\n",
    "posts['group'].value_counts().plot(kind='bar', title=\"Per group document counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One way to get a handle on a collection of documents (or \"corpus\") is to look at a wordcloud\n",
    "# Thankfully someone has written a little library to help us do that\n",
    "\n",
    "wc = WordCloud(background_color='white', width=1000, height=400, stopwords=[])\n",
    "wc.generate(\" \".join(t for t in posts[posts.group == 'rec.autos'].text)).to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oh dear that wasn't much use... of course common words completely dominate!\n",
    "# These are called \"stopwords\". It's common (if a little controversial these days...) to filter them out\n",
    "# The wordcloud library we're using supports that\n",
    "\n",
    "from wordcloud import STOPWORDS\n",
    "better_stopwords = STOPWORDS.union({'may', 'one', 'will', 'also'})\n",
    "wc = WordCloud(background_color='white', width=1000, height=400, stopwords=better_stopwords)\n",
    "wc.generate(\" \".join(t for t in posts[posts.group == 'rec.autos'].text)).to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, that's more like it! Let's eyeball all the groups\n",
    "\n",
    "for group in raw_posts.target_names:\n",
    "    print(\"Wordcloud for {}\".format(group))\n",
    "    display(wc.generate(\" \".join(t for t in posts[posts.group == group].text)).to_image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks okay, but comp.os.ms-windows.misc appears to be full of garbage\n",
    "# Let's cull it (rather crudely...) and take another look\n",
    "\n",
    "posts = posts[~posts.text.str.contains(\"AX\")]\n",
    "for group in raw_posts.target_names:\n",
    "    print(\"Wordcloud for {}\".format(group))\n",
    "    display(wc.generate(\" \".join(t for t in posts[posts.group == group].text)).to_image())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over to you: more windows on the data\n",
    "\n",
    "- What does the wordcloud for the whole corpus look like?\n",
    "- How many posts contain the word \"window\"? (Hint: look at how we removed posts containing \"AX\")\n",
    "- How do they split out across the groups? (Hint: look at how we drew the barchart for groups earlier)\n",
    "- (Tricky!) Can you filter out more junk posts?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to explore!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic extraction\n",
    "\n",
    "We're going to see if we can automatically infer a set of topics from the corpus. Obviously we'd expect these to be somehow related to the original newgroups, but perhaps there'll be some surprises?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's get the documents into a suitable form\n",
    "# Build a matrix by \"stacking\" the row vectors from SpaCy\n",
    "# Takes about 20 seconds...\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def vectorize(text):\n",
    "    # Get the SpaCy vector -- turning off other processing to speed things up\n",
    "    return nlp(unicode(text), disable=['parser', 'tagger', 'ner']).vector\n",
    "\n",
    "# Now we stack the vectors and normalize them\n",
    "# Inputs are typically called \"X\"\n",
    "X = normalize(np.stack(vectorize(t) for t in posts.text))\n",
    "print(\"X (the document matrix) has shape: {}\".format(X.shape))\n",
    "print(\"That means it has {} rows and {} columns\".format(X.shape[0], X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the vectors: 300D Glasses\n",
    "\n",
    "Our document vectors have 300 dimensions. That's quite difficult to visualize on a 2 dimensional screen!\n",
    "\n",
    "We're going to a use a standard technique called \"Principal Components Analysis\" (or PCA) to automatically reduce that to 2 dimensions, so we can get some insight into what's going on.\n",
    "\n",
    "You can read more about PCA at https://en.wikipedia.org/wiki/Principal_component_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit Learn ships with a neat PCA implementation\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X2 = pca.fit_transform(X)\n",
    "print(\"X2 shape is {}\".format(X2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay let's take a look at it via matplotlib\n",
    "\n",
    "def plot_groups(X, y, groups):\n",
    "    for group in groups:\n",
    "        plt.scatter(X[y == group, 0], X[y == group, 1], label=group, alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_groups(X2, posts.group, ('comp.os.ms-windows.misc', 'alt.atheism'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering the documents\n",
    "\n",
    "It looks like our vectors are doing something vaguely useful, in that there's a visual separate between groups.\n",
    "\n",
    "Now we'll use the standard \"k-means\" algorithm to automatically identify clusters within the data.\n",
    "\n",
    "You can read more about k-means at https://en.wikipedia.org/wiki/K-means_clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTERS = 20\n",
    "\n",
    "# First we fit the model...\n",
    "from sklearn.cluster import KMeans\n",
    "k_means = KMeans(n_clusters=CLUSTERS, random_state=1)\n",
    "k_means.fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we use it to predict clusters for each document...\n",
    "# Again it's common to use yhat for a predicted value -- although we wouldn't expect these to\n",
    "# correspond directly to the original groups\n",
    "yhat = k_means.predict(X)\n",
    "\n",
    "# Let's take a look at the distribution across classes\n",
    "plt.hist(yhat, bins=range(CLUSTERS))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be honest that's not looking very healthy -- ideally we'd see a more even distribution\n",
    "# Let's take a look at a couple of the big ones\n",
    "\n",
    "plot_groups(X2, yhat, (1,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay there are some definite (if rather blurry...) clusters there!\n",
    "# Let's have a look at how our clusters relate to the original groups\n",
    "def plot_cluster(c):\n",
    "    posts[yhat == c]['group'].value_counts().plot(kind='bar', title=\"Cluster #{}\".format(c))\n",
    "    plt.show()\n",
    "\n",
    "# Some are great matches...\n",
    "plot_cluster(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some are not so great a match, but sensible (why...?)\n",
    "plot_cluster(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some are just a bit random!\n",
    "plot_cluster(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the wordclouds...\n",
    "for c in range(CLUSTERS):\n",
    "    print(\"Wordcloud for category #{}\".format(c))\n",
    "    display(wc.generate(\" \".join(posts.text[yhat == c])).to_image())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, there's some confusion, but there are some really strong clusters there! Go us! But how could we do better?\n",
    "\n",
    "## Over to you: cluster buster\n",
    "\n",
    "\n",
    "\n",
    "- Whats going on with cluster #1?\n",
    "- Was ist der story with cluster #4?\n",
    "- Scroll way up and find where we're defining `CLUSTERS`. Try a smaller value (say 5). What effect does it have?\n",
    "- Try a larger (perhaps much larger...) value. What effect does it have?\n",
    "- (Tricky!) How could you decide automatically how many clusters to use? (Hint: take a look at the `bench_k_means()` method in http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to explore!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The workshop is dead long live the workshop!\n",
    "\n",
    "## Feedback\n",
    "\n",
    "I'd really love to get your feedback on this workshop (be it good, bad, pull request or bug report)! You can ping me at [joe.halliwell@gmail.com](mailto:joe.halliwell@gmail.com) or even tweet `@joehalliwell` if you're so inclined.\n",
    "\n",
    "## Other things to try\n",
    "\n",
    "We've really just scratched the surface of document clustering here. If you want to dig into the topic further (and dig out further topics), you might like to start by:\n",
    "\n",
    "- Using TFIDF vectors instead of SpaCy vectors: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "- Using Latent Dirichlet Allocation (LDA) instead of k-means: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
