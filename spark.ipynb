{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLLib Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out Jupyter\n",
    "\n",
    "1. Click the \"Help\" menu above, then \"User Interface Tour\"\n",
    "2. Click the grey cell below and hit `Ctrl + Enter` to run it\n",
    "3. Click on the `+` button above to make a new \"cell\". What's 1337 * 1337?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "%matplotlib inline\n",
    "print(\"Hello Ignite %s Commmunity!\" % random.choice([\"Big Data\", \"Machine Learning\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Spark usage\n",
    "\n",
    "Spark is basically the latest and greatest implementation of Hadoop MapReduce. It allows you to write infinitely scalable programs by focussing on **transforming** data stored in Resilient Distributed Datasets (RDDs) using functions with no side-effects.\n",
    "\n",
    "You can read more about the thinking behind MapReduce here: https://data-flair.training/blogs/hadoop-mapreduce-tutorial/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First we're going to snag a file from the web and save it to a local file\n",
    "# This kind of brevity is why they say Python comes with \"batteries included\"\n",
    "\n",
    "import urllib\n",
    "urllib.request.urlretrieve (\"https://github.com/joehalliwell/ml-workshop/raw/master/data/war-and-peace.txt\", \"war-and-peace.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we're going to load the file into spark and count the number of lines\n",
    "# We access Spark throught a global variable \"sc\" that's been magically created already...\n",
    "\n",
    "text = sc.textFile(\"war-and-peace.txt\")\n",
    "text.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Have a look at the first 10 lines\n",
    "\n",
    "text.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split each line into words -- try map() instead how does it differ?\n",
    "\n",
    "words = text.flatMap(lambda line: line.split())\n",
    "words.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we use a famous trick to count up each word. Can you see how it works?\n",
    "\n",
    "counts = words.map(lambda w: (w,1)).reduceByKey(lambda a,b: a + b)\n",
    "counts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# One of the ways Spark differs from classic MapReduce is by adding more powerful operations like sorting...\n",
    "\n",
    "counts.sortBy(lambda wordcount: -wordcount[1]).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark challenges!\n",
    "\n",
    "2. How can you modify the code to lower case words before counting?\n",
    "3. What's the median word?\n",
    "1. (Tricky) How many times does \"Russia\" appear?\n",
    "4. (Tricky) What are the most frequent word pairs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try your answers here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSQL DataFrames\n",
    "\n",
    "Spark DataFrames are like a scalable version of the in-memory Pandas dataframes we used in the last ML workshop. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Snag a data file\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/joehalliwell/ml-workshop/master/data/houses.csv\", \"houses.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is much simpler/nicer in later versions of Spark (the ones we use...)\n",
    "# We're accessing Spark's SQL functionality through another magic global variable\n",
    "\n",
    "houses = sqlContext.read \\\n",
    "    .format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"houses.csv\")\n",
    "    \n",
    "# Let's have a look at the first few items...\n",
    "\n",
    "houses.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hmm that wasn't very readable. But we can convert it to a Pandas dataframe to display it nicely.\n",
    "\n",
    "houses.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can perform LINQ-style manipulations...\n",
    "\n",
    "houses.select(\"id\", \"price\") \\\n",
    "    .withColumn(\"price_k\", houses[\"price\"] / 1000) \\\n",
    "    .limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "houses.where(houses[\"price\"] < 300000).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ...alternatively (and for interop with reporting tools) you can use SQL!\n",
    "\n",
    "sqlContext.registerDataFrameAsTable(df=houses, tableName=\"houses\")\n",
    "sqlContext.sql(\"SELECT bedrooms, COUNT(1) AS count FROM houses GROUP BY bedrooms\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm.... that's quite a lot of houses with zero bedrooms...\n",
    "\n",
    "### Dataframes challenges!\n",
    "\n",
    "1. How many waterfront houses are there? (Hint: you can use `groupBy().sum(\"columnname\")` to sum a column)\n",
    "2. What's the average number of bedrooms?\n",
    "3. Do waterfront houses have more or less bedrooms than average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try your answers here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning in Spark\n",
    "\n",
    "We're going to train a simple linear regression model to predict house prices.\n",
    "\n",
    "Spark ML's algorithms and implementations are all carefully designed to scale well with data volumes. The details are a bit complicated, but you can read more at https://spark.apache.org/docs/1.6.0/ml-classification-regression.html#regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's look at a toy example first\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.linalg import DenseVector\n",
    "import random\n",
    "\n",
    "# This is our toy data: y = 2 * x + 1 + noise\n",
    "noise = 10.0\n",
    "rows = [Row(label=2.0 * x + 1.0 + random.gauss(0, noise), features=DenseVector([x])) for x in range(100)]\n",
    "data = sqlContext.createDataFrame(rows)\n",
    "\n",
    "# Split into training and test sets\n",
    "train, test = data.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build and estimator and use it to fit a model\n",
    "estimator = LinearRegression()\n",
    "model = estimator.fit(train)\n",
    "\n",
    "# Run the model on unseen values and evaluate (by eye!)\n",
    "model.transform(test).toPandas().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "good = sqlContext.sql(\"SELECT * FROM houses WHERE bedrooms > 0 AND bathrooms > 0\")\n",
    "good = good.withColumn(\"label\", good[\"price\"].cast(\"double\"))\n",
    "print(\"Dropped {0} items\".format(houses.count() - good.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ML expects a dataframe with \"features\" and \"label\" columns\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "selector = VectorAssembler(\n",
    "    inputCols=[\"bedrooms\", \"bathrooms\", \"sqft_living\", \"sqft_lot\", \"floors\", \"lat\", \"long\"],\n",
    "   outputCol=\"features\")\n",
    "data = selector.transform(good).select(\"features\", \"label\")\n",
    "data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally we'll split the data into a training and test set\n",
    "\n",
    "train, test = data.randomSplit([0.8, 0.2])\n",
    "test.count(), train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we use the estimator to create a fitted model...\n",
    "estimator = LinearRegression()\n",
    "model = estimator.fit(train)\n",
    "print(\"Fitted {0}\".format(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ...which we can then use to generate a set of predictions\n",
    "\n",
    "predictions = model.transform(test)\n",
    "predictions.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we evaluate the predictions by looking at R-squared and RMSE\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(metricName=\"r2\")\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "print(\"R-squared (R2) on test data = %g\" % r2)\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root mean-squared error (RMSE) on test data = %g\" % rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark ML challenges!\n",
    "\n",
    "The results are frankly disappointing... it's over to you to make them better\n",
    "\n",
    "0. Try setting/tweaking some hyperparameters\n",
    "1. Try using a different regression model e.g. GBTRegressor https://spark.apache.org/docs/1.6.0/ml-classification-regression.html#regression\n",
    "2. Try a different feature set -- include more features (date?) or scale/transform existing ones https://spark.apache.org/docs/2.2.0/ml-features.html\n",
    "3. Experiment and share your R-squared with the group!\n",
    "4. Refactor the code/notebook to facilitate experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- MLlib example adapted from http://www.techpoweredmath.com/spark-dataframes-mllib-tutorial/#.WgTPv7y69hE\n",
    "- Interpreting R-squared http://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
